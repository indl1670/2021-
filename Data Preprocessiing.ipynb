{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge all csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** Merging multiple csv files into a single pandas dataframe **\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/tensorflow-2.1.0/lib/python3.7/site-packages/pandas/core/reshape/concat.py:304: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  sort=sort,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of dataframe:  1265657\n"
     ]
    }
   ],
   "source": [
    "# CSV Merge\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# merging the files\n",
    "files_joined = os.path.join('../STED', \"*.csv\")\n",
    "\n",
    "# Return a list of all joined files\n",
    "list_files = glob.glob(files_joined)\n",
    "\n",
    "print(\"** Merging multiple csv files into a single pandas dataframe **\")\n",
    "\n",
    "# Merge files by joining all files\n",
    "df = pd.concat(map(pd.read_csv, list_files), ignore_index=False)\n",
    "print(\"The number of dataframe: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1265657 entries, 0 to 235425\n",
      "Data columns (total 35 columns):\n",
      " #   Column                            Non-Null Count    Dtype  \n",
      "---  ------                            --------------    -----  \n",
      " 0   network_code                      1265613 non-null  object \n",
      " 1   receiver_code                     1265657 non-null  object \n",
      " 2   receiver_type                     1265657 non-null  object \n",
      " 3   receiver_latitude                 1265657 non-null  float64\n",
      " 4   receiver_longitude                1265657 non-null  float64\n",
      " 5   receiver_elevation_m              1265657 non-null  float64\n",
      " 6   p_arrival_sample                  1030231 non-null  float64\n",
      " 7   p_status                          1030231 non-null  object \n",
      " 8   p_weight                          1030057 non-null  float64\n",
      " 9   p_travel_sec                      1030231 non-null  float64\n",
      " 10  s_arrival_sample                  1030231 non-null  float64\n",
      " 11  s_status                          1030231 non-null  object \n",
      " 12  s_weight                          1030076 non-null  float64\n",
      " 13  source_id                         1030231 non-null  object \n",
      " 14  source_origin_time                1030231 non-null  object \n",
      " 15  source_origin_uncertainty_sec     690808 non-null   object \n",
      " 16  source_latitude                   1030231 non-null  float64\n",
      " 17  source_longitude                  1030231 non-null  float64\n",
      " 18  source_error_sec                  1011149 non-null  object \n",
      " 19  source_gap_deg                    932463 non-null   object \n",
      " 20  source_horizontal_uncertainty_km  1030227 non-null  object \n",
      " 21  source_depth_km                   1030231 non-null  object \n",
      " 22  source_depth_uncertainty_km       1030231 non-null  object \n",
      " 23  source_magnitude                  1030231 non-null  float64\n",
      " 24  source_magnitude_type             1030231 non-null  object \n",
      " 25  source_magnitude_author           1030231 non-null  object \n",
      " 26  source_mechanism_strike_dip_rake  1030231 non-null  object \n",
      " 27  source_distance_deg               1030231 non-null  float64\n",
      " 28  source_distance_km                1030231 non-null  float64\n",
      " 29  back_azimuth_deg                  1030231 non-null  float64\n",
      " 30  snr_db                            1030231 non-null  object \n",
      " 31  coda_end_sample                   1030231 non-null  object \n",
      " 32  trace_start_time                  1265657 non-null  object \n",
      " 33  trace_category                    1265657 non-null  object \n",
      " 34  trace_name                        1265657 non-null  object \n",
      "dtypes: float64(14), object(21)\n",
      "memory usage: 347.6+ MB\n"
     ]
    }
   ],
   "source": [
    "modified_file_csv = \"merge_chunks.csv\"\n",
    "df.to_csv(modified_file_csv, index=None)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lighten up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/EQTransformer/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (21) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total events in csv file: 200000\n",
      "total events selected: 165214\n",
      "0 / 165214\n",
      "1000 / 165214\n",
      "2000 / 165214\n",
      "3000 / 165214\n",
      "4000 / 165214\n",
      "5000 / 165214\n",
      "6000 / 165214\n",
      "7000 / 165214\n",
      "8000 / 165214\n",
      "9000 / 165214\n",
      "10000 / 165214\n",
      "11000 / 165214\n",
      "12000 / 165214\n",
      "13000 / 165214\n",
      "14000 / 165214\n",
      "15000 / 165214\n",
      "16000 / 165214\n",
      "17000 / 165214\n",
      "18000 / 165214\n",
      "19000 / 165214\n",
      "20000 / 165214\n",
      "21000 / 165214\n",
      "22000 / 165214\n",
      "23000 / 165214\n",
      "24000 / 165214\n",
      "25000 / 165214\n",
      "26000 / 165214\n",
      "27000 / 165214\n",
      "28000 / 165214\n",
      "29000 / 165214\n",
      "30000 / 165214\n",
      "31000 / 165214\n",
      "32000 / 165214\n",
      "33000 / 165214\n",
      "34000 / 165214\n",
      "35000 / 165214\n",
      "36000 / 165214\n",
      "37000 / 165214\n",
      "38000 / 165214\n",
      "39000 / 165214\n",
      "40000 / 165214\n",
      "41000 / 165214\n",
      "42000 / 165214\n",
      "43000 / 165214\n",
      "44000 / 165214\n",
      "45000 / 165214\n",
      "46000 / 165214\n",
      "47000 / 165214\n",
      "48000 / 165214\n",
      "49000 / 165214\n",
      "50000 / 165214\n",
      "51000 / 165214\n",
      "52000 / 165214\n",
      "53000 / 165214\n",
      "54000 / 165214\n",
      "55000 / 165214\n",
      "56000 / 165214\n",
      "57000 / 165214\n",
      "58000 / 165214\n",
      "59000 / 165214\n",
      "60000 / 165214\n",
      "61000 / 165214\n",
      "62000 / 165214\n",
      "63000 / 165214\n",
      "64000 / 165214\n",
      "65000 / 165214\n",
      "66000 / 165214\n",
      "67000 / 165214\n",
      "68000 / 165214\n",
      "69000 / 165214\n",
      "70000 / 165214\n",
      "71000 / 165214\n",
      "72000 / 165214\n",
      "73000 / 165214\n",
      "74000 / 165214\n",
      "75000 / 165214\n",
      "76000 / 165214\n",
      "77000 / 165214\n",
      "78000 / 165214\n",
      "79000 / 165214\n",
      "80000 / 165214\n",
      "81000 / 165214\n",
      "82000 / 165214\n",
      "83000 / 165214\n",
      "84000 / 165214\n",
      "85000 / 165214\n",
      "86000 / 165214\n",
      "87000 / 165214\n",
      "88000 / 165214\n",
      "89000 / 165214\n",
      "90000 / 165214\n",
      "91000 / 165214\n",
      "92000 / 165214\n",
      "93000 / 165214\n",
      "94000 / 165214\n",
      "95000 / 165214\n",
      "96000 / 165214\n",
      "97000 / 165214\n",
      "98000 / 165214\n",
      "99000 / 165214\n",
      "100000 / 165214\n",
      "101000 / 165214\n",
      "102000 / 165214\n",
      "103000 / 165214\n",
      "104000 / 165214\n",
      "105000 / 165214\n",
      "106000 / 165214\n",
      "107000 / 165214\n",
      "108000 / 165214\n",
      "109000 / 165214\n",
      "110000 / 165214\n",
      "111000 / 165214\n",
      "112000 / 165214\n",
      "113000 / 165214\n",
      "114000 / 165214\n",
      "115000 / 165214\n",
      "116000 / 165214\n",
      "117000 / 165214\n",
      "118000 / 165214\n",
      "119000 / 165214\n",
      "120000 / 165214\n",
      "121000 / 165214\n",
      "122000 / 165214\n",
      "123000 / 165214\n",
      "124000 / 165214\n",
      "125000 / 165214\n",
      "126000 / 165214\n",
      "127000 / 165214\n",
      "128000 / 165214\n",
      "129000 / 165214\n",
      "130000 / 165214\n",
      "131000 / 165214\n",
      "132000 / 165214\n",
      "133000 / 165214\n",
      "134000 / 165214\n",
      "135000 / 165214\n",
      "136000 / 165214\n",
      "137000 / 165214\n",
      "138000 / 165214\n",
      "139000 / 165214\n",
      "140000 / 165214\n",
      "141000 / 165214\n",
      "142000 / 165214\n",
      "143000 / 165214\n",
      "144000 / 165214\n",
      "145000 / 165214\n",
      "146000 / 165214\n",
      "147000 / 165214\n",
      "148000 / 165214\n",
      "149000 / 165214\n",
      "150000 / 165214\n",
      "151000 / 165214\n",
      "152000 / 165214\n",
      "153000 / 165214\n",
      "154000 / 165214\n",
      "155000 / 165214\n",
      "156000 / 165214\n",
      "157000 / 165214\n",
      "158000 / 165214\n",
      "159000 / 165214\n",
      "160000 / 165214\n",
      "161000 / 165214\n",
      "162000 / 165214\n",
      "163000 / 165214\n",
      "164000 / 165214\n",
      "165000 / 165214\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file_name = \"/home/STED/chunk2.hdf5\"\n",
    "csv_file = \"/home/STED/chunk2.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_file)\n",
    "print(f'total events in csv file: {len(df)}')\n",
    "\n",
    "# To distinguish between small earthquakes and noise\n",
    "df = df[((df.trace_category == 'earthquake_local') & (df.source_magnitude <= 2)) | (df.trace_category == 'noise')]\n",
    "print(f'total events selected: {len(df)}')\n",
    "df.to_csv(\"/home/STED/chunk2_modified.csv\")\n",
    "\n",
    "ev_list = df['trace_name'].to_list()\n",
    "\n",
    "dtfl = h5py.File(file_name, 'r')\n",
    "processed = h5py.File(\"/home/STED/chunk2_modified.hdf5\", 'w')\n",
    "grp = processed.create_group(\"data\")\n",
    "\n",
    "for c, evi in enumerate(ev_list):\n",
    "    dataset = dtfl.get('data/'+str(evi)) \n",
    "    dst = grp.create_dataset(str(evi), data=dataset)\n",
    "    \n",
    "    for key, value in dataset.attrs.items():\n",
    "        dst.attrs[key] = value\n",
    "        \n",
    "    if c % 1000 == 0:\n",
    "        print(str(c) + \" / \" + str(len(ev_list)))\n",
    "\n",
    "dtfl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 서버 Memory 부족 문제로 데이터셋의 수를 줄임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/EQTransformer/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3072: DtypeWarning: Columns (16,19,20,22) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 1000\n",
      "0 / 1000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "file_name = \"../STED/chunk2_modified.hdf5\"\n",
    "csv_file = \"../STED/chunk2_modified.csv\"\n",
    "modified_file_name = \"../STED/new_chunk2_modified.hdf5\"\n",
    "modified_file_csv = \"../STED/new_chunk2_modified.csv\"\n",
    "\n",
    "\n",
    "df = pd.read_csv(csv_file, index_col=0)\n",
    "\n",
    "num = list(range(1000))\n",
    "random.shuffle(num)\n",
    "\n",
    "new_df = df[0:1]\n",
    "\n",
    "for c, i in enumerate(num):\n",
    "    new_df = new_df.append(df[i:i+1])\n",
    "    if c % 1000 == 0:\n",
    "        print(str(c) + \" / \" + str(len(num)))\n",
    "\n",
    "new_df=new_df[1:]\n",
    "new_df.to_csv(modified_file_csv)\n",
    "\n",
    "ev_list = new_df['trace_name'].to_list()\n",
    "\n",
    "dtfl = h5py.File(file_name, 'r')\n",
    "processed = h5py.File(modified_file_name, 'w')\n",
    "grp = processed.create_group(\"data\")\n",
    "\n",
    "for c, evi in enumerate(ev_list):\n",
    "    dataset = dtfl.get('data/'+str(evi)) \n",
    "    dst = grp.create_dataset(str(evi), data=dataset)\n",
    "    \n",
    "    for key, value in dataset.attrs.items():\n",
    "        dst.attrs[key] = value\n",
    "        \n",
    "    if c % 1000 == 0:\n",
    "        print(str(c) + \" / \" + str(len(ev_list)))\n",
    "\n",
    "dtfl.close()\n",
    "processed.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EQTransformer",
   "language": "python",
   "name": "eqtransformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
